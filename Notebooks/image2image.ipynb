{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "42d9a7fc",
      "metadata": {
        "id": "42d9a7fc"
      },
      "source": [
        "# Search Similar Images\n",
        "\n",
        "Given an example image can we find similar images without the need of any labels? Leveraging Jina, we have the advantage that \n",
        "we don't need to use any labels or textual information about the images in order to build a search for similar images.\n",
        "\n",
        "In this tutorial we are going to create an image search system that retrieves similar images. We are going to\n",
        "use the test split of the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/data?select=test1.zip) dataset, which we\n",
        "will subsequently refer to as the pets dataset. It contains 12.5K images of cats and dogs. Now, we can define our\n",
        "problem as selecting an image of cat or dog, and getting back images of similar cats or dogs respectively.\n",
        "\n",
        "Jina searches semantically, and the results will vary depending on the neural network that we use for image encoding. Our\n",
        "task is to search for similar images so we will consider visually-similar images as semantically-related.\n",
        "\n",
        "## Build the Flow\n",
        "\n",
        "The solution uses a simple pipeline that can be subdivided into two steps:  **Index** and **Query**\n",
        "\n",
        "### Index\n",
        "\n",
        "To search something out of the full dataset, we first need to index the data. This means that we store the embeddings\n",
        "of all the images from the dataset in some form of storage. The images can be read as a numpy array which is then\n",
        "fed to the neural network of our choice. This neural network encodes the input images into some latent space which we call\n",
        "\"embeddings\". We then use an **Indexer** to store these embeddings in memory.\n",
        "\n",
        "### Query\n",
        "\n",
        "Once the data is indexed, i.e. our database is built, we simply need to feed our query (an image or set of\n",
        "images) to the model to encode it into embeddings and then use the **Indexer** to retrieve matching images. The matching\n",
        "can be based on any type of metric but without going deeper into this, we will focus only on Euclidean distance between\n",
        "two embeddings (corresponding to two images).\n",
        "\n",
        "We will use the **SimpleIndexer** Executor as\n",
        "our indexer (the one that stores and retrieves data). This Executor also returns the matching `Document` when we make\n",
        "a query. The search part is done using the built-in `match` function of `DocumentArray`. To encode the images into\n",
        "embeddings we will use our own Executor which uses the pre-trained 'ResNet101' model.\n",
        "\n",
        "\n",
        "## Insights\n",
        "\n",
        "Our first task is to wrap the image data as `Document`s and form a `DocumentArray`. This can be done easily with the\n",
        "following code snippet. `from_files` creates an iterator over a list of image paths and yields `Document`s:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d427dd",
      "metadata": {
        "id": "76d427dd"
      },
      "source": [
        "first, let's set up your kaggle account, download the credentials in the your `HOME/.kaggle/kaggle.json`. You can follow this [tutorial](https://www.kaggle.com/docs/api) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723fef9f",
      "metadata": {
        "id": "723fef9f"
      },
      "source": [
        "We are going to use the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/data?select=test1.zip) dataset so please first accept the competition rules on the kaggle website to be able to download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a67b775-c087-457a-88d2-dd2661f22e66",
      "metadata": {
        "tags": [],
        "id": "1a67b775-c087-457a-88d2-dd2661f22e66"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats --force\n",
        "!unzip dogs-vs-cats.zip -y\n",
        "!unzip train.zip -y\n",
        "!unzip test1.zip -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79429ef4",
      "metadata": {
        "id": "79429ef4"
      },
      "outputs": [],
      "source": [
        "from docarray import Document, DocumentArray\n",
        "\n",
        "image_format = \"jpg\"\n",
        "docs_array = DocumentArray.from_files(f\"test1/*.{image_format}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da79cae3",
      "metadata": {
        "id": "da79cae3"
      },
      "source": [
        "Once the image is loaded our next step is to encode these images into embeddings. As stated earlier you can use\n",
        "Executors from [Jina Hub](https://hub.jina.ai) off-the-shelf or you can define an Executor of your own in\n",
        "just a few steps. For this tutorial we will write our own Executor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f803a5",
      "metadata": {
        "id": "29f803a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from jina import Executor, requests\n",
        "from docarray import DocumentArray\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class ImageEncoder(Executor):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._embedder = models.resnet101(pretrained=True)\n",
        "        self._embedder.fx = (\n",
        "            nn.Identity()\n",
        "        )  # so that the output of the model is the embedding vector and not the classification logits\n",
        "\n",
        "    def _uri_to_torch_tensor(self, doc: Document):\n",
        "        return (\n",
        "            doc.load_uri_to_image_tensor()\n",
        "            .set_image_tensor_shape(shape=(224, 224))\n",
        "            .set_image_tensor_normalization()\n",
        "            .set_image_tensor_channel_axis(-1, 0)\n",
        "        )\n",
        "        \n",
        "\n",
        "    @requests\n",
        "    @torch.inference_mode()\n",
        "    def predict(self, docs: DocumentArray, **kwargs):\n",
        "        docs.apply(lambda d : self._uri_to_torch_tensor(d))  # load image from files and reshape make them torch tensors\n",
        "        embeds = self._embedder(torch.from_numpy(docs.tensors))  # embed with the resnet101\n",
        "        docs.embeddings = embeds  # store the embedding in the docs\n",
        "        del docs[:,'tensor'] # delete the tensors as we only want to have the embedding when indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d9202d",
      "metadata": {
        "id": "d4d9202d"
      },
      "source": [
        "To build an Encoder Executor we inherit the base `Executor` and use a decorator\n",
        "to define endpoints. As this `request` decorator is empty, this function will be called regardless of the\n",
        "endpoints invoked, i.e., on both the `/index` and `/search` endpoints. We\n",
        "leverage [torchvision](https://pytorch.org/vision/stable/index.html) to use the pre-trained `ResNet101` model for\n",
        "getting the embeddings. You can replace this model with any other pre-trained models of your choice. When this\n",
        "Executor is instantiated, the pre-trained weights are downloaded automatically. \n",
        "\n",
        "Finally, comes the storage/retrieval step. We do this with the **Indexer** Executor. You can use any of the\n",
        "available indexers on [Jina Hub](https://hub.jina.ai) or define your own. To create an **Indexer** you need to have two\n",
        "endpoints: `/index` and `/search`. For this tutorial we will define a `SimpleIndexer` which is [also available on jina\n",
        "Hub](https://hub.jina.ai/executor/zb38xlt4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda5b144",
      "metadata": {
        "id": "eda5b144"
      },
      "outputs": [],
      "source": [
        "from jina import Executor, requests\n",
        "\n",
        "class SimpleIndexer(Executor):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._index = DocumentArray(\n",
        "            storage='sqlite',\n",
        "            config={'connection': 'index.db','table_name':'image_to_image'},\n",
        "        )\n",
        "\n",
        "    @requests(on='/index')\n",
        "    def index(self, docs: DocumentArray, **kwargs):\n",
        "        self._index.extend(docs)\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def search(self, docs: DocumentArray, **kwargs):\n",
        "        docs.match(self._index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94117979",
      "metadata": {
        "tags": [],
        "id": "94117979"
      },
      "source": [
        "`SimpleIndexer` stores all the Documents with [sqlite backend](https://docarray.jina.ai/advanced/document-store/sqlite/?highlight=sqlite) on disk  when invoked with the `/index` endpoint. During the search\n",
        "Flow, it matches the query `Document` with the indexed `Document` using the built-in `match` function\n",
        "of `DocumentArray`.\n",
        "\n",
        "## Putting it all together in a Flow\n",
        "\n",
        "We will have one Flow defined for this tutorial. However, it handles requests to `/index` and `/search` differently by\n",
        "defining different endpoints using `requests` decorators. Below we see the Flow, which consists of an `Encoder` to encode\n",
        "the images as the first step, followed by an `Indexer` to store/retrieve data.\n",
        "\n",
        "So far we saw individual components of the Flow and how to define them. Next comes putting all of this together in a Flow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66487c04",
      "metadata": {
        "id": "66487c04"
      },
      "outputs": [],
      "source": [
        "from jina import Flow\n",
        "\n",
        "f = (\n",
        "    Flow(cors=True, port_expose=12345, protocol=\"http\")\n",
        "    .add(uses=ImageEncoder, name=\"Encoder\")\n",
        "    .add(uses=SimpleIndexer, name=\"Indexer\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac2d810f",
      "metadata": {
        "id": "ac2d810f"
      },
      "source": [
        "### Start the Flow and Index data\n",
        "\n",
        "here we only index 1000 images, if you want to index more you should consider using a GPU (see [this section](https://docs.jina.ai/how-to/gpu-executor/?highlight=gpu) to learn how to use Executor with GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82677a61",
      "metadata": {
        "id": "82677a61"
      },
      "outputs": [],
      "source": [
        "with f:\n",
        "    f.post(\"/index\", inputs=docs_array.shuffle()[0:10],show_progress=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ec459e8",
      "metadata": {
        "id": "2ec459e8"
      },
      "source": [
        "### Query from Python\n",
        "\n",
        "Keeping the server running we can start a simple client to make a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82103721",
      "metadata": {
        "id": "82103721"
      },
      "outputs": [],
      "source": [
        "from jina import Client, Document\n",
        "from docarray import DocumentArray\n",
        "\n",
        "\n",
        "def print_matches(resp):  # the callback function invoked when task is done\n",
        "    resp.docs.plot_image_sprites()\n",
        "    for doc in resp.docs:\n",
        "        for idx, d in enumerate(doc.matches[:3]):  # print top-3 matches\n",
        "            print(f'[{idx}]{d.scores[\"cosine\"].value:2f}')\n",
        "\n",
        "        DocumentArray(doc.matches[:3]).plot_image_sprites()\n",
        "\n",
        "with f:\n",
        "    c = Client(protocol=\"http\", port=12345)  # connect to localhost:12345\n",
        "    c.post(\"/search\", inputs=docs_array[0:2], on_done=print_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72fd8ab",
      "metadata": {
        "id": "d72fd8ab"
      },
      "source": [
        "## Results\n",
        "\n",
        "The returned response contains the matching `Document` which in turn contains the `uri` of the images. Below we can see the\n",
        "returned matching images of the query as well as the cosine similarity score:\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "image2image.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}