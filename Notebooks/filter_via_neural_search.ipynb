{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52f6e63a-38df-4ca8-8f85-d8d1a389f96f",
      "metadata": {
        "id": "52f6e63a-38df-4ca8-8f85-d8d1a389f96f"
      },
      "source": [
        "# Filter Table Rows by Attributes\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Tip:</b> Find the full source code and run TagsHasher on Jina Hub: https://hub.jina.ai.</div>\n",
        "\n",
        "\n",
        "\n",
        "Big news, you can use Jina to filter table rows by their attributes! Such an amazing feature that only exists since... 47 years ago, aka SQL!\\\n",
        "Jina as a neural search framework surely won't implement a SQL database from scratch. The question here is: is it possible to leverage what we learned about neural search (embedding, indexing, nearest-neighbour matching) to enable similar feature like SQL, e.g. filter, select?\n",
        "\n",
        "Yes! Jina can do this. In this article, I will give you a walkthrough on how to filter the tabular data using Jina and without SQL (also no GPT-3). Let's call this mini-project the neuretro-SQL.\n",
        "\n",
        "## Feature hashing\n",
        "\n",
        "The first thing you want to learn is feature hashing.\n",
        "\n",
        "In general, feature hashing is a great way to embed **unbounded** number of features into fixed-size vectors. We will leverage the same idea here to embed the columns of the tabular data into fixed-size vectors.\n",
        "\n",
        "## Load CSV as DocumentArray\n",
        "\n",
        "Let's look at an example CSV file. Here I use a [film dataset](https://perso.telecom-paristech.fr/eagan/class/igr204/data/film.csv) that looks like the following:  \n",
        "\n",
        "![film dataset](https://github.com/jina-ai/tutorial-notebooks/blob/main/tabular/film-dataset.png?raw=1)\n",
        "\n",
        "Let's load the data from the web and put them into a DocumentArray:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install docarray['Full']"
      ],
      "metadata": {
        "id": "OkF_1XVhzFo9"
      },
      "id": "OkF_1XVhzFo9",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jina"
      ],
      "metadata": {
        "id": "0NY1QStv0R-R",
        "outputId": "85b114f0-b2c2-4c2a-a1b7-b45b462ea7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "0NY1QStv0R-R",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jina\n",
            "  Downloading jina-3.2.7.tar.gz (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting websockets\n",
            "  Downloading websockets-10.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting pathspec\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting docker\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jina) (1.21.5)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (from jina) (0.75.0)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jina) (2.23.0)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.7/dist-packages (from jina) (0.17.6)\n",
            "Collecting protobuf>=3.19.1\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from jina) (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from jina) (3.6.0)\n",
            "Collecting uvloop\n",
            "  Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-multipart in /usr/local/lib/python3.7/dist-packages (from jina) (0.0.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (from jina) (12.0.1)\n",
            "Requirement already satisfied: grpcio>=1.33.1 in /usr/local/lib/python3.7/dist-packages (from jina) (1.44.0)\n",
            "Requirement already satisfied: docarray>=0.9.10 in /usr/local/lib/python3.7/dist-packages (from jina) (0.9.16)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from jina) (21.3)\n",
            "Collecting lz4<3.1.2\n",
            "  Downloading lz4-3.1.1-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 48.9 MB/s \n",
            "\u001b[?25hCollecting aiostream\n",
            "  Downloading aiostream-0.4.4.tar.gz (32 kB)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.33.1->jina) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->jina) (3.0.7)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (6.0.2)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.3 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (3.10.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->jina) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->jina) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->jina) (2.21)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jina) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jina) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jina) (3.0.4)\n",
            "Requirement already satisfied: starlette==0.17.1 in /usr/local/lib/python3.7/dist-packages (from fastapi->jina) (0.17.1)\n",
            "Requirement already satisfied: anyio<4,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.17.1->fastapi->jina) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.0.0->starlette==0.17.1->fastapi->jina) (1.2.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich->jina) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->jina) (2.6.1)\n",
            "Requirement already satisfied: asgiref>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]->jina) (3.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]->jina) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]->jina) (0.13.0)\n",
            "Collecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\n",
            "Collecting httptools>=0.4.0\n",
            "  Downloading httptools-0.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (406 kB)\n",
            "\u001b[K     |████████████████████████████████| 406 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting watchgod>=0.6\n",
            "  Downloading watchgod-0.8.1-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: jina, aiostream\n",
            "  Building wheel for jina (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jina: filename=jina-3.2.7-py3-none-any.whl size=349312 sha256=cdcf51071be67425ac934d436de45422d8f0cfc90ac8753266d5aefd28002c45\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/f8/45/5976736323cd3b948a9cc2908c8906e69b47b423caf82ce3c6\n",
            "  Building wheel for aiostream (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aiostream: filename=aiostream-0.4.4-py3-none-any.whl size=35648 sha256=d450e1975b92e10042ae471f61b990bdaf9d30d2a60e15925b292182927db36c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/21/19/0141f098b792ee743e77c609bc0d42e7742b1f66db58f56cd9\n",
            "Successfully built jina aiostream\n",
            "Installing collected packages: frozenlist, yarl, websockets, websocket-client, watchgod, uvloop, pyyaml, python-dotenv, httptools, asynctest, async-timeout, aiosignal, protobuf, pathspec, lz4, docker, cryptography, aiostream, aiohttp, aiofiles, jina\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: lz4\n",
            "    Found existing installation: lz4 4.0.0\n",
            "    Uninstalling lz4-4.0.0:\n",
            "      Successfully uninstalled lz4-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed aiofiles-0.8.0 aiohttp-3.8.1 aiosignal-1.2.0 aiostream-0.4.4 async-timeout-4.0.2 asynctest-0.13.0 cryptography-36.0.2 docker-5.0.3 frozenlist-1.3.0 httptools-0.4.0 jina-3.2.7 lz4-3.1.1 pathspec-0.9.0 protobuf-3.19.4 python-dotenv-0.19.2 pyyaml-6.0 uvloop-0.16.0 watchgod-0.8.1 websocket-client-1.3.1 websockets-10.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da6e43bb-b10e-434d-ab0c-41481ff13c44",
      "metadata": {
        "id": "da6e43bb-b10e-434d-ab0c-41481ff13c44"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "# Load some online CSV file dataset\n",
        "src = Document(\n",
        "    uri='https://perso.telecom-paristech.fr/eagan/class/igr204/data/film.csv'\n",
        ").load_uri_to_text('iso8859')\n",
        "da = DocumentArray.from_csv(io.StringIO(src.text), dialect='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4272feda-6658-42a9-81e5-3ff550fb1913",
      "metadata": {
        "id": "4272feda-6658-42a9-81e5-3ff550fb1913"
      },
      "source": [
        "Here we use Document API to download the data, convert it into the right charset, and load it via our CSV API as a DocumentArray.\n",
        "\n",
        "Looks like we got 1660 Documents in total, let's take one sample from it and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fea808b1-b67f-4936-8327-1244b26ad1d4",
      "metadata": {
        "id": "fea808b1-b67f-4936-8327-1244b26ad1d4",
        "outputId": "7313a12e-b0be-4baa-e5dc-68f364eb12b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"6204269ba583bc05cd704565b1b28188\", \"parent_id\": null, \"granularity\": null, \"adjacency\": null, \"blob\": null, \"tensor\": null, \"mime_type\": null, \"text\": null, \"weight\": null, \"uri\": null, \"tags\": {\"Popularity\": \"14\", \"Subject\": \"Drama\", \"Length\": \"94\", \"Director\": \"Malick, Terrence\", \"Awards\": \"No\", \"Title\": \"Days of Heaven\", \"Actor\": \"Gere, Richard\", \"Actress\": \"Adams, Brooke\", \"*Image\": \"NicholasCage.png\", \"Year\": \"1978\"}, \"offset\": null, \"location\": null, \"embedding\": null, \"modality\": null, \"evaluations\": null, \"scores\": null, \"chunks\": null, \"matches\": null}\n"
          ]
        }
      ],
      "source": [
        "print(da[5].to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36eec8f9-1eb7-41e9-bacd-beb22a1b5342",
      "metadata": {
        "id": "36eec8f9-1eb7-41e9-bacd-beb22a1b5342"
      },
      "source": [
        "It looks like this Document has two important non-empty attributes `id` and `tags`, and all values in `tags` correspond to the column value we have in the CSV data.\\\n",
        "Now our task is clear: we want to filter Documents from this DocumentArray according to their values in `.tags`, but no SQL, pure Jina, pure neural search.\n",
        "\n",
        "## Embed columns as vectors\n",
        "\n",
        "To embed columns into vectors, we first notice that each \"column-item\" in `.tags` is actually a `Tuple[str, Any]` pair. The first part, a string, represents the column title, e.g. \"Actor\", \"Actress\", \"Director\".\\\n",
        "To hash such values, you can use the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9a040269-78da-4362-848a-58666444a5eb",
      "metadata": {
        "id": "9a040269-78da-4362-848a-58666444a5eb",
        "outputId": "96055ed0-e6b3-43a7-c3b7-9f9aebc3f95c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import hashlib\n",
        "\n",
        "h = lambda x: int(hashlib.md5(str(x).encode('utf-8')).hexdigest(), base=16) % 256\n",
        "\n",
        "h('Actor')\n",
        "h('Director')\n",
        "h('Length')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2486137-916e-4c11-8eb3-c52af3ad05d4",
      "metadata": {
        "id": "a2486137-916e-4c11-8eb3-c52af3ad05d4"
      },
      "source": [
        "Now that we have indices, the actual value on that index, namely the `Any` part of that `Tuple[str, Any]` pair needs some extra thought.\n",
        "\n",
        "First, some values are numbers like integers or floats, they are a good hash by themselves, so they do not need another hash function.\\\n",
        "Boolean values are the same, 0 and 1 are pretty representative. Strings can be handled in the same way above. What about lists, tuples and dicts?\\\n",
        "We can serialize them into JSON strings and then apply our string hash.\n",
        "\n",
        "The final hash function looks like the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9adc092e-a38e-4982-90e2-5aa569db9967",
      "metadata": {
        "id": "9adc092e-a38e-4982-90e2-5aa569db9967"
      },
      "outputs": [],
      "source": [
        "def _any_hash(self, v):\n",
        "    try:\n",
        "        return int(v)  # parse int parameter\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return float(v)  # parse float parameter\n",
        "        except ValueError:\n",
        "            if not v:\n",
        "                # ignore it when the parameter is empty\n",
        "                return 0\n",
        "            if isinstance(v, str):\n",
        "                v = v.strip()\n",
        "                if v.lower() in {'true', 'yes'}:  # parse boolean parameter\n",
        "                    return 1\n",
        "                if v.lower() in {'false', 'no'}:\n",
        "                    return 0\n",
        "            if isinstance(v, (tuple, dict, list)):\n",
        "                v = json.dumps(v, sort_keys=True)\n",
        "\n",
        "    return int(self.hash(str(v).encode('utf-8')).hexdigest(), base=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea325b5-07e7-4a65-b924-92b1f0db3ac3",
      "metadata": {
        "id": "cea325b5-07e7-4a65-b924-92b1f0db3ac3"
      },
      "source": [
        "If you apply this directly, you will get extremely big integers on the embedding values.\n",
        "So big that you don't even want to look at or store it (for numerical and stability reason).\n",
        "\n",
        "So we need to bound it.\\\n",
        "We can introdoce the variables `n_dim` and `max_val` to bound the dimension of our emeddings \"horizontally\" and \"vertically\", respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13dea809-3fa0-4fcc-9432-f257bd0a3627",
      "metadata": {
        "id": "13dea809-3fa0-4fcc-9432-f257bd0a3627"
      },
      "outputs": [],
      "source": [
        "n_dim: int = 256\n",
        "max_val: int = 65536"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff1747c4-f41c-4aed-8ad0-15d166c8e3d7",
      "metadata": {
        "id": "ff1747c4-f41c-4aed-8ad0-15d166c8e3d7"
      },
      "source": [
        "Here we give a larger number to `max_val` then to `n_dim`.\\\n",
        "This is because the likelihood of a collision happens on vertical direction is in general much higher than on horizontal direction (otherwise, it implies there are more variants on the column name than on the column value, which then suggests the table-maker to simply \"transpose\" the whole table for better readability).\n",
        "\n",
        "The final embedding procedure is then very simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "49538466-eb5c-47fb-afcb-446b3e1bbc63",
      "metadata": {
        "id": "49538466-eb5c-47fb-afcb-446b3e1bbc63"
      },
      "outputs": [],
      "source": [
        "def encode(self, docs: DocumentArray, **kwargs):\n",
        "    for idx, doc in enumerate(docs):\n",
        "        if doc.tags:\n",
        "            idxs, data = [], []  # sparse\n",
        "            table = np.zeros(self.n_dim)  # dense\n",
        "            for k, v in doc.tags.items():\n",
        "                h = self._any_hash(k)\n",
        "                sign_h = np.sign(h)\n",
        "                col = h % self.n_dim\n",
        "                val = self._any_hash(v)\n",
        "                sign_v = np.sign(val)\n",
        "                val = val % self.max_val\n",
        "                idxs.append((0, col))\n",
        "                val = sign_h * sign_v * val\n",
        "                data.append(val)\n",
        "                table[col] += val\n",
        "\n",
        "            if self.sparse:\n",
        "                doc.embedding = csr_matrix(\n",
        "                    (data, zip(*idxs)), shape=(1, self.n_dim)\n",
        "                )\n",
        "            else:\n",
        "                doc.embedding = table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67d1ff05-314e-4d59-9003-f7a60a853883",
      "metadata": {
        "id": "67d1ff05-314e-4d59-9003-f7a60a853883"
      },
      "source": [
        "## Put it all together\n",
        "\n",
        "Now you can put your code together into one Executor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f9d8f823-f51a-4312-bd51-a8aee5ff4caf",
      "metadata": {
        "id": "f9d8f823-f51a-4312-bd51-a8aee5ff4caf"
      },
      "outputs": [],
      "source": [
        "from docarray import Document, DocumentArray\n",
        "from jina import Executor, requests\n",
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "class TagsHasher(Executor):\n",
        "    def __init__(self, n_dim: int = 256, max_val: int = 65536, sparse: bool = False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_dim = n_dim\n",
        "        self.max_val = max_val\n",
        "        self.hash = hashlib.md5\n",
        "        self.sparse = sparse\n",
        "        \n",
        "    def _any_hash(self, v):\n",
        "        try:\n",
        "            return int(v)  # parse int parameter\n",
        "        except ValueError:\n",
        "            try:\n",
        "                return float(v)  # parse float parameter\n",
        "            except ValueError:\n",
        "                if not v:\n",
        "                    # ignore it when the parameter is empty\n",
        "                    return 0\n",
        "                if isinstance(v, str):\n",
        "                    v = v.strip()\n",
        "                    if v.lower() in {'true', 'yes'}:  # parse boolean parameter\n",
        "                        return 1\n",
        "                    if v.lower() in {'false', 'no'}:\n",
        "                        return 0\n",
        "                if isinstance(v, (tuple, dict, list)):\n",
        "                    v = json.dumps(v, sort_keys=True)\n",
        "        return int(self.hash(str(v).encode('utf-8')).hexdigest(), base=16)\n",
        "        \n",
        "    @requests\n",
        "    def encode(self, docs: DocumentArray, **kwargs):\n",
        "        if self.sparse:\n",
        "            from scipy.sparse import csr_matrix\n",
        "            \n",
        "        for idx, doc in enumerate(docs):\n",
        "            if doc.tags:\n",
        "                idxs, data = [], []  # sparse\n",
        "                table = np.zeros(self.n_dim)  # dense\n",
        "                for k, v in doc.tags.items():\n",
        "                    h = self._any_hash(k)\n",
        "                    sign_h = np.sign(h)\n",
        "                    col = h % self.n_dim\n",
        "                    val = self._any_hash(v)\n",
        "                    sign_v = np.sign(val)\n",
        "                    val = val % self.max_val\n",
        "                    idxs.append((0, col))\n",
        "                    val = sign_h * sign_v * val\n",
        "                    data.append(val)\n",
        "                    table[col] += val\n",
        "\n",
        "                if self.sparse:\n",
        "                    doc.embedding = csr_matrix(\n",
        "                        (data, zip(*idxs)), shape=(1, self.n_dim)\n",
        "                    )\n",
        "                else:\n",
        "                    doc.embedding = table\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5212f96f-b590-4f66-b646-0b73a65d1bb6",
      "metadata": {
        "id": "5212f96f-b590-4f66-b646-0b73a65d1bb6"
      },
      "source": [
        "Let's encode our loaded DocumentArray:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ce38132a-f213-49e3-a577-0544813d29a3",
      "metadata": {
        "id": "ce38132a-f213-49e3-a577-0544813d29a3"
      },
      "outputs": [],
      "source": [
        "th = TagsHasher()\n",
        "th.encode(da)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce69d4c-4fa7-405d-a5a8-90fe0aab0186",
      "metadata": {
        "id": "4ce69d4c-4fa7-405d-a5a8-90fe0aab0186"
      },
      "source": [
        "Now let's build some filters as Document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73ce0f9f-c7be-48e6-8fb3-12ef67798933",
      "metadata": {
        "id": "73ce0f9f-c7be-48e6-8fb3-12ef67798933"
      },
      "outputs": [],
      "source": [
        "filters = [\n",
        "    {\"Subject\": \"Comedy\"},\n",
        "    {\"Year\": 1987},\n",
        "    {\"Subject\": \"Comedy\", \"Year\": 1987}\n",
        "]\n",
        "\n",
        "qa = DocumentArray([Document(tags=f) for f in filters])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87df8278-d45b-463a-a67b-86a1e8d4176d",
      "metadata": {
        "id": "87df8278-d45b-463a-a67b-86a1e8d4176d"
      },
      "source": [
        "Encode the filter with `TagsHasher` to get the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "784fe8ea-b4e3-466d-8e5d-cd0c27ce191e",
      "metadata": {
        "id": "784fe8ea-b4e3-466d-8e5d-cd0c27ce191e"
      },
      "outputs": [],
      "source": [
        "th.encode(qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ba4149cd-252e-4176-a4ec-a479391b6eaf"
      },
      "outputs": [],
      "source": [
        "qa.match(da, limit=5, exclude_self=True, metric='jaccard', use_scipy=True)"
      ],
      "id": "ba4149cd-252e-4176-a4ec-a479391b6eaf"
    },
    {
      "cell_type": "markdown",
      "id": "549113f7-154d-400e-afff-1fe94e307ad6",
      "metadata": {
        "id": "549113f7-154d-400e-afff-1fe94e307ad6"
      },
      "source": [
        "Now that we have embeddings for both indexed docs `da` (i.e. our film CSV table), and the query docs `qa` (our filters), we can use `.match` function to find nearest neighbours.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb57775-8c90-4781-8190-a81055aa4fb6",
      "metadata": {
        "id": "ffb57775-8c90-4781-8190-a81055aa4fb6"
      },
      "source": [
        "Note that here I use Jaccard distance instead of the  cosine distance. This is because the closeness of the value on each feature is meaningless, as the value is the result of a hash function. Whereas in `FeatureHashser`'s example, the value represents the term frequency of a word, so it was meaningful there. This needs to be kept in mind when using `TagsHasher`.\n",
        "\n",
        "Finally, let's see some results. Here I only print top-5 matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cf6edde5-a2ef-424e-8ae4-e0c96f25cac4",
      "metadata": {
        "id": "cf6edde5-a2ef-424e-8ae4-e0c96f25cac4",
        "outputId": "23fdddc7-0ca2-40f8-d92c-7ff472e45653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my filter is: {'Subject': 'Comedy'}\n",
            "{'Popularity': '82', 'Subject': 'Comedy', 'Length': '', 'Director': '', 'Awards': 'No', 'Title': 'Valkenvania', 'Actor': 'Chase, Chevy', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1990'}\n",
            "{'Popularity': '28', 'Subject': 'Comedy', 'Length': '', 'Director': '', 'Awards': 'No', 'Title': 'Secret War of Harry Frigg, The', 'Actor': 'Newman, Paul', 'Actress': '', '*Image': 'paulNewman.png', 'Year': '1968'}\n",
            "{'Popularity': '56', 'Subject': 'Comedy', 'Length': '', 'Director': '', 'Awards': 'No', 'Title': 'Best of Eddie Murphy, Saturday Night Live, The', 'Actor': 'Murphy, Eddie', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1989'}\n",
            "{'Popularity': '29', 'Subject': 'Comedy', 'Length': '', 'Director': 'Fellini, Federico', 'Awards': 'No', 'Title': 'Ginger & Fred', 'Actor': 'Mastroianni, Marcello', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1993'}\n",
            "{'Popularity': '14', 'Subject': 'Comedy', 'Length': '60', 'Director': '', 'Awards': 'No', 'Title': 'Joe Piscopo New Jersey Special', 'Actor': 'Piscopo, Joe', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "my filter is: {'Year': 1987}\n",
            "{'Popularity': '75', 'Subject': 'Music', 'Length': '50', 'Director': '', 'Awards': 'No', 'Title': 'Madonna Live, The Virgin Tour', 'Actor': '', 'Actress': 'Madonna', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '14', 'Subject': 'Comedy', 'Length': '60', 'Director': '', 'Awards': 'No', 'Title': 'Joe Piscopo New Jersey Special', 'Actor': 'Piscopo, Joe', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '25', 'Subject': 'Drama', 'Length': '95', 'Director': '', 'Awards': 'No', 'Title': 'Hearts of Fire', 'Actor': 'Everett, Rupert', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '41', 'Subject': 'Drama', 'Length': '', 'Director': 'Cimino, Michael', 'Awards': 'No', 'Title': 'Sicilian, The', 'Actor': 'Lambert, Christopher', 'Actress': 'Sukowa, Barbara', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '87', 'Subject': 'Action', 'Length': '98', 'Director': 'Rosenthal, Rick', 'Awards': 'No', 'Title': 'Russkies', 'Actor': 'Hubley, Whip', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "my filter is: {'Subject': 'Comedy', 'Year': 1987}\n",
            "{'Popularity': '14', 'Subject': 'Comedy', 'Length': '60', 'Director': '', 'Awards': 'No', 'Title': 'Joe Piscopo New Jersey Special', 'Actor': 'Piscopo, Joe', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '51', 'Subject': 'Comedy', 'Length': '90', 'Director': 'Murphy, Eddie', 'Awards': 'No', 'Title': 'Eddie Murphy Raw', 'Actor': 'Murphy, Eddie', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '23', 'Subject': 'Comedy', 'Length': '', 'Director': 'Gottlieb, Michael', 'Awards': 'No', 'Title': 'Mannequin', 'Actor': 'McCarthy, Andrew', 'Actress': 'Cattrall, Kim', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '37', 'Subject': 'Comedy', 'Length': '120', 'Director': 'Levinson, Barry', 'Awards': 'No', 'Title': 'Good Morning, Vietnam', 'Actor': 'Williams, Robin', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n",
            "{'Popularity': '69', 'Subject': 'Comedy', 'Length': '86', 'Director': 'Schultz, Michael', 'Awards': 'No', 'Title': 'Disorderlies', 'Actor': 'Boys, The Fat', 'Actress': '', '*Image': 'NicholasCage.png', 'Year': '1987'}\n"
          ]
        }
      ],
      "source": [
        "for d in qa:\n",
        "    print('my filter is:', d.tags)\n",
        "    for m in d.matches:\n",
        "        print(m.tags)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "filter-via-neural-search.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}