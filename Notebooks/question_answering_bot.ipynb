{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5020e076",
      "metadata": {
        "tags": [],
        "id": "5020e076"
      },
      "source": [
        "# Open-Domain Question-Answering on Long Document\n",
        "\n",
        "The following tutorial will take you through a solution of to question-answering on long documents. \n",
        "This is an inherently difficult task, due to the fuzziness of human language and the infinite number of questions one could ask.\n",
        "\n",
        "One way to solve this is by predicting answers using a neural network that was trained on pairs of questions and their corresponding answers. In many cases such a dataset is not available, like in the case of most software documentation. Let's say we want to build a chatbot to answer questions about the Jina documentation. What if I told you that there is a way to reframe this task as a search problem and that this would alleviate the need for a large dataset of matching questions and answers?\n",
        "\n",
        "How, you ask? *Let me explain!*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecaa526-76d3-4cf7-8b77-ba99d843b0a4",
      "metadata": {
        "tags": [],
        "id": "0ecaa526-76d3-4cf7-8b77-ba99d843b0a4"
      },
      "source": [
        "## Overview \n",
        "Our approach to the problem leverages the [Doc2query method](https://arxiv.org/pdf/1904.08375.pdf), which, form a piece of text, predicts different questions the text could potentially answer. For example, given a sentence such as `Jina is an open source framework for neural search.`, the model predicts questions such as `What is Jina?` or `Is Jina open source?`.\n",
        "\n",
        "The idea here is to predict several questions for every part of the original text document, in our case the Jina documentation. Then we use an encoder to create a vector representation for each of the predicted questions. These representations are stored and provide the index for our body of text. When a user prompts the bot with a question, we encode it in the same way we encoded our generated questions. Now we can run a similarity search on the encodings. The encoding of the user's query is compared with the encodings of generated questions, in our index to find the closes match. \n",
        "\n",
        "Once the matching answer is found, we can return it to the user. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950d11d2-472a-4972-a585-f860de2a80a2",
      "metadata": {
        "id": "950d11d2-472a-4972-a585-f860de2a80a2"
      },
      "source": [
        "The final application will consist of two `Flow`, one for the indexing of the questions\n",
        "![8indexing-flow](https://github.com/jina-ai/tutorial-notebooks/blob/main/question-answering-bot/indexing-flow.svg?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ea0a94-446d-41ad-8e23-14534da2da5d",
      "metadata": {
        "id": "12ea0a94-446d-41ad-8e23-14534da2da5d"
      },
      "source": [
        "and the second one for the querying\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bddbe45-e9f2-4140-8514-d9efd466f4e1",
      "metadata": {
        "id": "4bddbe45-e9f2-4140-8514-d9efd466f4e1"
      },
      "source": [
        "![querying-flow](https://github.com/jina-ai/tutorial-notebooks/blob/main/question-answering-bot/querying-flow.svg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295e53fa-17e6-43f3-a5f1-9706c181b0da",
      "metadata": {
        "id": "295e53fa-17e6-43f3-a5f1-9706c181b0da"
      },
      "source": [
        "## ⏰ Installing & Importing Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b5a83ce-df27-45ac-9891-ed1d0f6406bd",
      "metadata": {
        "id": "1b5a83ce-df27-45ac-9891-ed1d0f6406bd"
      },
      "outputs": [],
      "source": [
        "!pip install jina transformers>=4.9.1 matplotlib torch>=1.9.0 sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c183fb6f",
      "metadata": {
        "id": "c183fb6f"
      },
      "source": [
        "\n",
        "## Building the Executor to Generate Potential Questions \n",
        "\n",
        "The first `Executor` that we implement, is the `QuestionGenerator`. It is a wrapper around the model that predicts potential questions, which a given piece of text can answer.\n",
        "\n",
        "Apart from that, it just loops over all provided parts of input text. After potential questions are predicted for each of the inputs, they are stored as `chunks` alongside the original text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987db5b9",
      "metadata": {
        "id": "987db5b9"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from docarray import DocumentArray, Document\n",
        "from jina import Executor, requests\n",
        "\n",
        "class QuestionGenerator(Executor):\n",
        "    @requests\n",
        "    def doc2query(self, docs: DocumentArray, **kwargs):\n",
        "        \"\"\"Generates potential questions for each answer\"\"\"\n",
        "\n",
        "        # Load pretrained doc2query models\n",
        "        self._tokenizer = T5Tokenizer.from_pretrained(\n",
        "            'castorini/doc2query-t5-base-msmarco'\n",
        "        )\n",
        "        self._model = T5ForConditionalGeneration.from_pretrained(\n",
        "            'castorini/doc2query-t5-base-msmarco'\n",
        "        )\n",
        "\n",
        "        for d in docs:\n",
        "            input_ids = self._tokenizer.encode(d.content, return_tensors='pt')\n",
        "            # Generte potential queries for each piece of text\n",
        "            outputs = self._model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_length=64,\n",
        "                do_sample=True,\n",
        "                num_return_sequences=10,\n",
        "            )\n",
        "            # Decode the outputs ot text and store them\n",
        "            for output in outputs:\n",
        "                question = self._tokenizer.decode(\n",
        "                    output, skip_special_tokens=True\n",
        "                ).strip()\n",
        "                d.chunks.append(Document(text=question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ae534a",
      "metadata": {
        "id": "46ae534a"
      },
      "source": [
        "Notice that this Executor puts generated questions into a Document's `chunks`. When matching queries to text snippets\n",
        "you will have to take that into account.\n",
        "\n",
        "We try to give credit where credit is due and want to mention the paper,that introduced the doc2query approach [here](https://arxiv.org/pdf/1904.08375.pdf).\n",
        "\n",
        "## Building the Encoder\n",
        "The next step is to build the `Executor`, which we will use to create vector representations from human-readable text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1833fd9f",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "1833fd9f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from docarray import DocumentArray, Document\n",
        "from jina import Executor, requests\n",
        "\n",
        "class TextEncoder(Executor):\n",
        "    def __init__(self, parameters: dict = {'traversal_paths': '@r'}, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model = SentenceTransformer(\n",
        "            'paraphrase-mpnet-base-v2', device='cpu', cache_folder='.'\n",
        "        )\n",
        "        self.parameters = parameters\n",
        "\n",
        "    @requests(on=['/search', '/index'])\n",
        "    def encode(self, docs: DocumentArray, **kwargs):\n",
        "        \"\"\"Wraps encoder from sentence-transformers package\"\"\"\n",
        "        traversal_paths = self.parameters.get('traversal_paths')\n",
        "        target = docs[traversal_paths]\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            embeddings = self.model.encode(target.texts)\n",
        "            target.embeddings = embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38177f78-5e61-41c8-baee-b290a2f6081d",
      "metadata": {
        "id": "38177f78-5e61-41c8-baee-b290a2f6081d"
      },
      "source": [
        "Similar to the `QuestionGenerator` the `TextEncoder` is simply a wrapper around the SentenceTransformer from the sentence_transformer package. When provided with a `DocumentArray` containing text, it will encode the text of each element and store the result in the `embedding` attribute it creates.\n",
        "\n",
        "When querying you can use this Executor to simply embed all top level Documents themselves.\n",
        "When indexing, however, you will have to make sure to embed the `chunks` of all documents - that is where the generated questions are, after all.\n",
        "\n",
        "Now let's move on to the last part and create the indexer. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f943dd",
      "metadata": {
        "tags": [],
        "id": "85f943dd"
      },
      "source": [
        "\n",
        "## Putting it Together with the Indexer\n",
        "The indexer is the only one of our `Executor`s that can handle more than one task. \n",
        "Namely, the indexing and the search.\n",
        "\n",
        "When it is used to perform indexing, `index()` is called. This stores all provided documents, together with their embeddings, as a `DocumentArrayMemmap`. \n",
        "\n",
        "However, when the `SimpleIndexer` is used to handle an incoming query, the `search()` function is called, it performs similarity search and ranks the results. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa9bdf1",
      "metadata": {
        "id": "afa9bdf1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "from docarray import DocumentArray\n",
        "\n",
        "class SimpleIndexer(Executor):\n",
        "    \"\"\"Simple indexer class\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._docs = DocumentArray(storage='sqlite',\n",
        "                                    config={'connection': os.path.join(self.workspace, 'index.db'),\n",
        "                                            'table_name': 'indexed_docs'})\n",
        "\n",
        "    @requests(on='/index')\n",
        "    def index(self, docs: 'DocumentArray', **kwargs):\n",
        "        # Stores the index in attribute\n",
        "        if docs:\n",
        "            self._docs.extend(docs)\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def search(self, docs: 'DocumentArray', **kwargs):\n",
        "        \"\"\"Append best matches to each document in docs\"\"\"\n",
        "\n",
        "        docs.match(  # Match query agains the index using cosine similarity\n",
        "            self._docs['@c'][...],\n",
        "            metric='cosine',\n",
        "            normalization=(1, 0),\n",
        "            limit=100,\n",
        "            traversal_rdarray='c,',\n",
        "        )\n",
        "\n",
        "        for d in docs:\n",
        "            match_similarity = defaultdict(float)\n",
        "            # For each match\n",
        "            for m in d.matches:\n",
        "                # Get cosine similarity\n",
        "                match_similarity[m.parent_id] += m.scores['cosine'].value\n",
        "\n",
        "            sorted_similarities = sorted(\n",
        "                match_similarity.items(), key=lambda v: v[1], reverse=True\n",
        "            )\n",
        "\n",
        "            # Rank matches by similarity and collect them\n",
        "            d.matches.clear()\n",
        "            for k, _ in sorted_similarities:\n",
        "                m = Document(self._docs[k], copy=True)\n",
        "                d.matches.append(m)\n",
        "                # Only return top 10 answers\n",
        "                if len(d.matches) >= 10:\n",
        "                    break\n",
        "            # Remove embedding as it is not needed anymore\n",
        "            d.pop('embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c562ce2e-ff8b-4398-ae00-2febc59195b5",
      "metadata": {
        "id": "c562ce2e-ff8b-4398-ae00-2febc59195b5"
      },
      "source": [
        "Document is the basic data type that Jina operates with\n",
        "Executor processes a DocumentArray in-place\n",
        "Jina uses the concept of a flow to tie different executors together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac16a7a9",
      "metadata": {
        "id": "ac16a7a9"
      },
      "source": [
        "We have now seen how to implement a question-answering bot using Jina without the need for a large dataset of matching questions and answers. In practice, we would need to experiment with several parameters, such as the initial extraction of answers from the original text. In this tutorial, we made the assumption that every sentence will be one potential answer. However, in reality, it is likely that some user queries will require multiple sentences or complete paragraphs to answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a3e882-2797-42e0-a827-c1d6f9fb1a7b",
      "metadata": {
        "id": "26a3e882-2797-42e0-a827-c1d6f9fb1a7b"
      },
      "source": [
        "## Puting everything together in a Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce6c2a8-a6c1-4116-8137-59d6742f7749",
      "metadata": {
        "tags": [],
        "id": "8ce6c2a8-a6c1-4116-8137-59d6742f7749"
      },
      "source": [
        "### Indexing the text document \n",
        "Let's imagine we extracted a bunch of sentences from Jina's documentation and stored them in a `DocumentArray`, as shown below. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694287ec-f5fd-435a-b999-da3d65189372",
      "metadata": {
        "id": "694287ec-f5fd-435a-b999-da3d65189372"
      },
      "source": [
        "As described in the last section, we first need to predict potential questions for each of the elements in the `DocumentArray`. Then we have to use another model to create vector encodings from the predicted questions. Finally, we store them as the index. \n",
        "\n",
        "At this point we have enough information to start defining our `Flows`.\n",
        "\n",
        "*Without further ado, let's build!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e63295c-eaa5-4bbe-870f-9b72625fba31",
      "metadata": {
        "id": "6e63295c-eaa5-4bbe-870f-9b72625fba31"
      },
      "outputs": [],
      "source": [
        "from docarray import DocumentArray, Document\n",
        "from jina import Executor, requests, Flow\n",
        "\n",
        "example_sentences = [\n",
        "    'Document is the basic data type that Jina operates with',\n",
        "    'Executor processes a DocumentArray in-place',\n",
        "    'Jina uses the concept of a flow to tie different executors together'\n",
        "]\n",
        "\n",
        "docs = DocumentArray([Document(content=sentence) for sentence in example_sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64be02f5-00b2-4534-b6fe-c9f89876f065",
      "metadata": {
        "id": "64be02f5-00b2-4534-b6fe-c9f89876f065"
      },
      "outputs": [],
      "source": [
        "indexing_flow = (\n",
        "    Flow()\n",
        "    # Generate potential questions using doc2query\n",
        "    .add(\n",
        "        name='question_transformer',\n",
        "        uses=QuestionGenerator,\n",
        "    )\n",
        "    # Encode the generated questions\n",
        "    .add(\n",
        "        name='text_encoder',\n",
        "        uses=TextEncoder,\n",
        "        uses_with={'parameters': {'traversal_paths': '@c'}},\n",
        "    )\n",
        "    # Index answers and generated questions\n",
        "    .add(\n",
        "        name='simple_indexer',\n",
        "        uses=SimpleIndexer,\n",
        "        uses_metas={'workspace': 'workspace'}\n",
        "    )\n",
        ")\n",
        "\n",
        "indexing_flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51202ec-6e8e-40b6-92d6-423146397caa",
      "metadata": {
        "id": "b51202ec-6e8e-40b6-92d6-423146397caa"
      },
      "outputs": [],
      "source": [
        "with indexing_flow:\n",
        "    # Run the indexing on all extracted sentences\n",
        "    indexing_flow.post(on='/index', inputs=docs,show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9d6888-52b5-42c1-9de3-6f3af80272e1",
      "metadata": {
        "id": "be9d6888-52b5-42c1-9de3-6f3af80272e1"
      },
      "source": [
        "This Flow first generates questions for each input, then encodes these generated questions, and lastly saves them to disk.\n",
        "\n",
        "In order to ensure that you are able to load these questions and embeddings at query time, you need to set a *workspace*\n",
        "for your indexer, using `uses_metas={'workspace': 'path/to/workspace'}` in `flow.add()`.\n",
        "This is where the data will be stored, and later you will set the same working directory for the query Flow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65413dea-c717-478d-9ab3-14997d158093",
      "metadata": {
        "id": "65413dea-c717-478d-9ab3-14997d158093"
      },
      "source": [
        "### Searching of the user's query against the index\n",
        "\n",
        "After having defined the `Flow` for indexing our document, we are now ready to work on answering user queries. Incoming queries also need to be encoded. For that, we use the same encoder that we used for encoding our generated questions. Then we need `SimpleIndexer` to perform similarity search, in order to retrieve generated questions and eventually answers the query. \n",
        "\n",
        "The flow for searching is much simpler than the one for indexing and looks like this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b727a5e8-3b6b-4053-8640-02b7ffe918d7",
      "metadata": {
        "id": "b727a5e8-3b6b-4053-8640-02b7ffe918d7"
      },
      "outputs": [],
      "source": [
        "example_queries = ['What is a Document?',\n",
        "                   'How are DocumentArrays processed in an Executor?',\n",
        "                   'How are different Executors tied together in Jina?']\n",
        "user_queries = DocumentArray(Document(text=q) for q in example_queries)\n",
        "\n",
        "query_flow = (\n",
        "    Flow()\n",
        "    # Create vector representations from query\n",
        "    .add(name='query_transformer', uses=TextEncoder,)\n",
        "    # Use encoded question to search our index\n",
        "    .add(\n",
        "        name='simple_indexer',\n",
        "        uses=SimpleIndexer,\n",
        "        uses_metas={'workspace': 'workspace'}\n",
        "    )\n",
        ")\n",
        "\n",
        "query_flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e839c39a-608c-49c2-9ad5-ca32dea9037f",
      "metadata": {
        "id": "e839c39a-608c-49c2-9ad5-ca32dea9037f"
      },
      "outputs": [],
      "source": [
        "def print_top_match(resp):\n",
        "    print('--------------------QA--------------------------')\n",
        "    for d in resp.docs:\n",
        "        print(d.text)\n",
        "        print(d.matches.texts[0])\n",
        "        print('------------------------------------------------')\n",
        "\n",
        "\n",
        "with query_flow:\n",
        "    # Run question through the query flow and return answer\n",
        "    query_flow.post(\n",
        "        on='/search', inputs=user_queries, return_results=True, on_done=print_top_match\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c632cb4-2f9e-4f7b-a533-023190370476",
      "metadata": {
        "id": "6c632cb4-2f9e-4f7b-a533-023190370476"
      },
      "source": [
        "here you go ! You have defined your first QA system with jina "
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "question-answering-bot.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}