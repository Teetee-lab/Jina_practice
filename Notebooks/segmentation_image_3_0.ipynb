{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqN-od3lX0h1"
      },
      "source": [
        "# Find Small Images Inside Large Images\n",
        "\n",
        "In this tutorial we will build an image search engine capable of finding small images inside bigger ones. This requires a different architecture than typical image search engines since we need to perform object detection.\n",
        "\n",
        "As we want to find small images inside big images, simply encoding both the indexed images and the query image and matching will not work. Imagine that you have the following big image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgKvMwrQX0h5"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/jina-ai/tutorial-notebooks/blob/main/small-images-inside-big-images/cat-bird.jpeg?raw=1\" width=\"300\" align=\"left\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSOGde-oX0h5"
      },
      "source": [
        "It contains a scene with a cat in the background, a bird and a few other items in the scene.\n",
        "\n",
        "Now let’s suppose that the query image is a simple bird.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/jina-ai/tutorial-notebooks/blob/main/small-images-inside-big-images/bird.jpeg?raw=1\" width=\"100\" align=\"left\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaPAk5YhX0h6"
      },
      "source": [
        "Encoding the query image will generate embeddings that effectively represent it. However, it’s not easy to build an encoder that effectively represents the big image, since it contains a complex scene with different objects. The embeddings will not be representative enough and therefore we need to think about a better approach.\n",
        "\n",
        "Can you think of another solution ?\n",
        "\n",
        "Encoding a complex image is not easy, but if we can identify objects inside the big image and encode each one of them. It will result in better, more representative embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcQZrntABHo0"
      },
      "source": [
        "## ⏰ Installing & Importing Dependencies\n",
        "\n",
        "We will start this tutorial by installing the necessary ***pip*** dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfPNTR2WLD9e"
      },
      "outputs": [],
      "source": [
        "! pip install Pillow jina==3.0 torch==1.9.0 torchvision==0.10.0 transformers==4.9.1 yolov5==5.0.7 lmdb==1.2.1 matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS9TR-HpdPSe"
      },
      "outputs": [],
      "source": [
        "# clean up\n",
        "! rm -rf workspace images query\n",
        "! rm data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdr0WZ2jBO5S"
      },
      "source": [
        "**Downloading and unzipping data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6edmcg9-CkZ"
      },
      "outputs": [],
      "source": [
        "! wget https://open-images.s3.eu-central-1.amazonaws.com/data.zip\n",
        "! unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1c9QgBTBWYy"
      },
      "source": [
        "## **Executors**\n",
        "In this section, we will start developing the necessary executors, for both query and index flows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAH11pDzBnAU"
      },
      "source": [
        "### **CLIPImageEncoder**\n",
        "This encoder encodes an image into embeddings using the CLIP model. \n",
        "We want an executor that loads the CLIP model and encodes it during the query and index flows. \n",
        "\n",
        "Our executor should:\n",
        "* support both **GPU** and **CPU**: That's why we will provision the `device` parameter and use it when encoding.\n",
        "* be able to process documents in batches in order to use our resources effectively: To do so, we will use the parameter `batch_size`\n",
        "* be able to encode the full image during the query flow and encode only chunks during the index flow: This can be achieved with `traversal_paths` and method `DocumentArray.batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3PkqQvHLnFA"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from jina import Executor, requests\n",
        "from docarray import DocumentArray\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel\n",
        "\n",
        "\n",
        "class CLIPImageEncoder(Executor):\n",
        "    \"\"\"Encode image into embeddings using the CLIP model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_model_name_or_path: str = 'openai/clip-vit-base-patch32',\n",
        "        device: str = 'cpu',\n",
        "        batch_size: int = 32,\n",
        "        traversal_paths: str = '@r',\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.traversal_paths = traversal_paths\n",
        "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
        "\n",
        "        self.device = device\n",
        "        self.preprocessor = CLIPFeatureExtractor.from_pretrained(\n",
        "            pretrained_model_name_or_path\n",
        "        )\n",
        "        self.model = CLIPModel.from_pretrained(self.pretrained_model_name_or_path)\n",
        "        self.model.to(self.device).eval()\n",
        "\n",
        "    @requests\n",
        "    def encode(self, docs: DocumentArray, parameters: dict, **kwargs):\n",
        "        if docs is None:\n",
        "            return\n",
        "\n",
        "        document_batches_generator =  DocumentArray(\n",
        "            filter(\n",
        "                lambda x: x.tensor is not None,\n",
        "                docs[parameters.get('traversal_paths', self.traversal_paths)],\n",
        "            )\n",
        "        ).batch(batch_size=parameters.get('batch_size', self.batch_size))\n",
        "\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for batch_docs in document_batches_generator:\n",
        "                tensors_batch = [d.tensor for d in batch_docs]\n",
        "                tensor = self._generate_input_features(tensors_batch)\n",
        "                \n",
        "\n",
        "                embeddings = self.model.get_image_features(**tensor)\n",
        "                embeddings = embeddings.cpu().numpy()\n",
        "\n",
        "                batch_docs.embeddings = embeddings\n",
        "\n",
        "\n",
        "    def _generate_input_features(self, images):\n",
        "        input_tokens = self.preprocessor(\n",
        "            images=images,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        input_tokens = {\n",
        "            k: v.to(torch.device(self.device)) for k, v in input_tokens.items()\n",
        "        }\n",
        "        return input_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBdhRATrWOmd"
      },
      "source": [
        "### **YoloV5Segmenter**\n",
        "Since we want to retrieve small images in bigger images, the technique that we will heavily rely on is segmenting. Basicly, we want to perform object detection on the indexed images. This will generate bounding boxes around objects detected inside the images. The detected objects will be extracted and added as chunks to the original documents.\n",
        "BTW, guess what is the state-of-the-art object detection model ?\n",
        "Right, we will use YoloV5.\n",
        "\n",
        "\n",
        "Our **YoloV5Segmenter** should be able to load the `ultralytics/yolov5` model from Torch hub, otherwise, load a custom model. To achieve this, the executor accepts parameter `model_name_or_path` which will be used when loading. We will implement the method `load` which checks if the model exists in the the Torch Hub, otherwise, loads it as a custom model.\n",
        "\n",
        "For our use case, we will just rely on `yolov5s` (small version of `yolov5`). Of course, for better quality, you can choose a more complicated model or your custom model.\n",
        "\n",
        "Furtheremore, we want **YoloV5Segmenter** to support both **GPU** and **CPU** and it should be able to process in batches. Again, this is as simple as adding parameters `device` and `batch_size` and using them during segmenting.\n",
        "\n",
        "To perform segmenting, we will implement method `_segment_docs` which performs the following steps:\n",
        "1. For each batch (a batch consists of several images), use the model to get predictions for each image\n",
        "2. Each prediction of an image can contain several detections (because yolov5 will extract as much bounding boxes as possible, along with their confidence scores). We will filter out detections whose scores are below the `confidence_threshold` to keep a good quality.\n",
        "\n",
        "Each detection is actually 2 points -top left (x1, y1) and bottom right (x2, y2)- a confidence score and a class. We will not use the class of the detection, but it can be useful in other search applications.\n",
        "\n",
        "3. With the detections that we have, we create crops (using the 2 points returned). Finally, we add these crops to image documents as chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8iT8BqZNyzL"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Iterable, Optional\n",
        "\n",
        "import torch\n",
        "from jina import Executor, requests\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "\n",
        "class YoloV5Segmenter(Executor):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path: str = 'yolov5s',\n",
        "        confidence_threshold: float = 0.3,\n",
        "        batch_size: int = 32,\n",
        "        device: str = 'cpu',\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model_name_or_path = model_name_or_path\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        if device != 'cpu' and not device.startswith('cuda'):\n",
        "            self.logger.error('Torch device not supported. Must be cpu or cuda!')\n",
        "            raise RuntimeError('Torch device not supported. Must be cpu or cuda!')\n",
        "        if device == 'cuda' and not torch.cuda.is_available():\n",
        "            self.logger.warning(\n",
        "                'You tried to use GPU but torch did not detect your'\n",
        "                'GPU correctly. Defaulting to CPU. Check your CUDA installation!'\n",
        "            )\n",
        "            device = 'cpu'\n",
        "        self.device = torch.device(device)\n",
        "        self.model = self._load(self.model_name_or_path)\n",
        "\n",
        "    @requests\n",
        "    def segment(\n",
        "        self, docs: Optional[DocumentArray] = None, parameters: Dict = {}, **kwargs\n",
        "    ):\n",
        "\n",
        "        if docs:\n",
        "            document_batches_generator = DocumentArray(\n",
        "            filter(\n",
        "                lambda x: x.tensor is not None,\n",
        "                docs,\n",
        "            )\n",
        "        ).batch(batch_size=parameters.get('batch_size', self.batch_size))\n",
        "            self._segment_docs(document_batches_generator, parameters=parameters)\n",
        "\n",
        "    def _segment_docs(self, document_batches_generator: Iterable, parameters: Dict):\n",
        "        with torch.no_grad():\n",
        "            for document_batch in document_batches_generator:\n",
        "                images = [d.tensor for d in document_batch]\n",
        "                predictions = self.model(\n",
        "                    images,\n",
        "                    size=640,\n",
        "                    augment=False,\n",
        "                ).pred\n",
        "\n",
        "                for doc, prediction in zip(document_batch, predictions):\n",
        "                    for det in prediction:\n",
        "                        x1, y1, x2, y2, conf, cls = det\n",
        "                        if conf < parameters.get(\n",
        "                            'confidence_threshold', self.confidence_threshold\n",
        "                        ):\n",
        "                            continue\n",
        "                        crop = doc.tensor[int(y1): int(y2), int(x1): int(x2), :]\n",
        "                        doc.chunks.append(Document(tensor=crop))\n",
        "\n",
        "    def _load(self, model_name_or_path):\n",
        "        if model_name_or_path in torch.hub.list('ultralytics/yolov5'):\n",
        "            return torch.hub.load(\n",
        "                'ultralytics/yolov5', model_name_or_path, device=self.device\n",
        "            )\n",
        "        else:\n",
        "            return torch.hub.load(\n",
        "                'ultralytics/yolov5', 'custom', model_name_or_path, device=self.device\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSi4hBTmV8mR"
      },
      "source": [
        "**Indexers**\n",
        "After developing the encoder, we will need 2 kinds of indexers: \n",
        "1. SimpleIndexer: This indexer will take care of storing chunks of images. It also supports vector similarity search which is important to match small query images against segments of original images.\n",
        "\n",
        "2. LMDBStorage: LMDB is a simple memory-mapped transactional key-value store. It is convenient for this example because we can use it to store original indexed images so that we can retrieve them later. We will use it to create LMDBStorage which offers 2 functionnalities: indexing documents and retrieving documents by ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQG41eMxdVE8"
      },
      "source": [
        "**SimpleIndexer**\n",
        "To implement SimpleIndexer, we can leverage jina's `DocumentArray` with SQLite as storage backend. This will allow us to persist Documents in a database and allow Nearest Neighbors search using `DocumentArray.match`.\n",
        "Our indexer will create an instance of `DocumentArray` when it's initialized. We also specify the backend configuration like the connection and table name.\n",
        "\n",
        "To index, we implement the method `index` which is bound to the index flow. It's as simple as extending the received docs.\n",
        "\n",
        "On the other hand, for search, we implement the method `search`. We bind it to the query flow using the decorator `@requests(on='/search')`.\n",
        "In jina, searching for query documents can be done by adding the results to the `matches` attribute of each query document. Since docs is a `DocumentArray` we can use method `match` to match query against the indexed documents.\n",
        "Read more about `match` [here](https://docarray.jina.ai/fundamentals/documentarray/matching/).\n",
        "There's another detail here: We already indexed documents before search, but we need to match query documents against chunks of the indexed images. \n",
        "To do so, we can loop over chunks using the following selector: `self._index['@c']`. This will provide an iterator over chunks for the `match` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu2whtYOL7mI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "from jina import Executor, requests\n",
        "from docarray import DocumentArray\n",
        "\n",
        "\n",
        "class SimpleIndexer(Executor):\n",
        "\n",
        "    FILE_NAME = 'index.db'\n",
        "    TABLENAME = 'storage'\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self._index = DocumentArray(\n",
        "            storage='sqlite',\n",
        "            config={\n",
        "                'connection': os.path.join(self.workspace, SimpleIndexer.FILE_NAME),\n",
        "                'table_name': SimpleIndexer.TABLENAME,\n",
        "            },\n",
        "        )  # with customize config\n",
        "\n",
        "    @requests(on='/index')\n",
        "    def index(\n",
        "        self,\n",
        "        docs: 'DocumentArray',\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if docs:\n",
        "            self._index.extend(docs)\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def search(\n",
        "        self,\n",
        "        docs: 'DocumentArray',\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if not docs:\n",
        "            return\n",
        "\n",
        "        docs.match(self._index['@c'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp4t_8ZPgDEW"
      },
      "source": [
        "**LMDBStorage**\n",
        "\n",
        "In order to implement the LMDBStorage, we need the following parts:\n",
        "\n",
        "**I. Handler**\n",
        "\n",
        "This will be a context manager that we will use when we access our LMDB database. We will create it as a standalone class.\n",
        "\n",
        "\n",
        "**II. LMDBStorage constructor**\n",
        "\n",
        "The constructor should initialize a few attributes:\n",
        "* the `map_size` of the database\n",
        "* the index file: again, to keep things clean, we will store the index file inside the workspace folder. Therefore we can use the `workspace` attribute.\n",
        "\n",
        "\n",
        "**III. `LMDBStorage.index`**\n",
        "\n",
        "In order to index documents, we first start a transaction (so that our Storage executor is ACID-compliant). Then, we traverse the root documents. Finally, each document is serialized to bytes and then added to the database (the key is the document ID)\n",
        "\n",
        "\n",
        "**IV. `LMDBStorage.search`**\n",
        "\n",
        "Unlike search in the SimpleIndexer, we only wish to get the matched Documents by ID and return them. Actually, the matched documents will be empty and will only contain the IDs. The goal is to return full matched documents using IDs.\n",
        "To accomplish this, again, we start a transaction, traverse the matched documents, get each matched document by ID and use the results to fill our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRlx1YVbfiVc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List\n",
        "\n",
        "import lmdb\n",
        "from jina import Executor, requests\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "\n",
        "class _LMDBHandler:\n",
        "    def __init__(self, file, map_size):\n",
        "        # see https://lmdb.readthedocs.io/en/release/#environment-class for usage\n",
        "        self.file = file\n",
        "        self.map_size = map_size\n",
        "\n",
        "    @property\n",
        "    def env(self):\n",
        "        return self._env\n",
        "\n",
        "    def __enter__(self):\n",
        "        self._env = lmdb.Environment(\n",
        "            self.file,\n",
        "            map_size=self.map_size,\n",
        "            subdir=False,\n",
        "            readonly=False,\n",
        "            metasync=True,\n",
        "            sync=True,\n",
        "            map_async=False,\n",
        "            mode=493,\n",
        "            create=True,\n",
        "            readahead=True,\n",
        "            writemap=False,\n",
        "            meminit=True,\n",
        "            max_readers=126,\n",
        "            max_dbs=0,  # means only one db\n",
        "            max_spare_txns=1,\n",
        "            lock=True,\n",
        "        )\n",
        "        return self._env\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if hasattr(self, '_env'):\n",
        "            self._env.close()\n",
        "\n",
        "\n",
        "class LMDBStorage(Executor):\n",
        "    def __init__(\n",
        "        self,\n",
        "        map_size: int = 1048576000,  # in bytes, 1000 MB\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.map_size = map_size\n",
        "        self.file = os.path.join(self.workspace, 'db.lmdb')\n",
        "        if not os.path.exists(self.workspace):\n",
        "            os.makedirs(self.workspace)\n",
        "\n",
        "    def _handler(self):\n",
        "        return _LMDBHandler(self.file, self.map_size)\n",
        "\n",
        "    @requests(on='/index')\n",
        "    def index(self, docs: DocumentArray, parameters: Dict, **kwargs):\n",
        "        if docs is None:\n",
        "            return\n",
        "        with self._handler() as env:\n",
        "            with env.begin(write=True) as transaction:\n",
        "                for d in docs:\n",
        "                    transaction.put(d.id.encode(), d.to_bytes())\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def search(self, docs: DocumentArray, parameters: Dict, **kwargs):\n",
        "        if docs is None:\n",
        "            return\n",
        "\n",
        "        with self._handler() as env:\n",
        "            with env.begin(write=True) as transaction:\n",
        "                for doc in docs:\n",
        "                    for match_doc in doc.matches:\n",
        "                      id = match_doc.id\n",
        "                      serialized_doc = Document.from_bytes(transaction.get(match_doc.id.encode()))\n",
        "                      match_doc.copy_from(serialized_doc)\n",
        "                      match_doc.id = id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KH6EdlQlGZq"
      },
      "source": [
        "**SimpleRanker**\n",
        "\n",
        "You might think why do we need a ranker at all ?\n",
        "Actually, a ranker is needed because we will be matching small query images against chunks of parent documents. But how can we get back to parent documents (aka full images) given the chunks ? And what if 2 chunks belonging to the same parent are matched ?\n",
        "We can solve this by aggregating the similarity scores of chunks that belong to the same parent (using an aggregation method, in our case, will be the `min` value).\n",
        "So, for each query document, we perform the following:\n",
        "\n",
        "1. We create an empty collection of parent scores. This collection will store, for each parent, a list of scores of its chunk documents.\n",
        "2. For each match, since it's originally a chunk document, we can retrieve its `parent_id`. And it's also a match document so we get its match score and add that value to the parent socres collection.\n",
        "3. After processing all matches, we need to aggregate the scores of each parent using the `min` metric.\n",
        "4. Finally, using the aggregated score values of parents, we can create a new list of matches (this time consisting of parents, not chunks). We also need to sort the matches list by aggregated scores.\n",
        "\n",
        "When query documents exit the SimpleRanker, they now have matches consisting of parent documents. However, parent documents just have IDs. That's why, during the previous steps, we created LMDBStorage in order to actually retrieve parent documents by IDs and fill them with data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Amlx0_nL2uV"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict, Iterable, Optional\n",
        "\n",
        "from jina import Executor, requests\n",
        "from docarray import Document, DocumentArray\n",
        "from docarray.score import NamedScore\n",
        "\n",
        "\n",
        "class SimpleRanker(Executor):\n",
        "    def __init__(\n",
        "        self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.metric = 'cosine'\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def rank(\n",
        "        self, docs: DocumentArray, parameters: Dict = {}, **kwargs\n",
        "    ):\n",
        "        if docs is None:\n",
        "            return\n",
        "\n",
        "        for doc in docs:\n",
        "            parents_scores = defaultdict(list)\n",
        "            for m in doc.matches:\n",
        "                parents_scores[m.parent_id].append(m.scores[self.metric].value)\n",
        " \n",
        "            # Aggregate match scores for parent document and\n",
        "            # create doc's match based on parent document of matched chunks\n",
        "            matches = []\n",
        "            for match_parent_id, scores in parents_scores.items():\n",
        "                score = min(scores)\n",
        "                matches.append(\n",
        "                    Document(id=match_parent_id, scores={self.metric: NamedScore(value=score)})\n",
        "                )\n",
        "\n",
        "            # Sort the matches\n",
        "            doc.matches.clear()\n",
        "            doc.matches.extend(sorted(matches, key=lambda d: d.scores[self.metric].value))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjXL3OXKKIgL"
      },
      "source": [
        "### Enabling GPU\n",
        "Usually, indexing takes some time because YoloV5Segmenter and CLIPImageEncoder can be slow. However, you can speed up indexing by enabling GPU. To do so, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "* Navigate to Edit→Notebook Settings\n",
        "* Select GPU from the Hardware Accelerator drop-down\n",
        "* Change the device to 'cuda' in the following line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-d-YFJlJ9ST"
      },
      "outputs": [],
      "source": [
        "device = 'cpu' # change to 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYbgiCezowH_"
      },
      "source": [
        "## Indexing\n",
        "Now, after creating executors, it's time to use them in order to build an index Flow and index our data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUMN5kbXpKou"
      },
      "source": [
        "### Building the index Flow\n",
        "We create a Flow object and add executors one after the other with the right parameters:\n",
        "\n",
        "1. YoloV5Segmenter: We should also specify the device\n",
        "2. CLIPImageEncoder: It also receives the device parameter. And since we only encode the chunks, we specify  `'traversal_paths': 'c'`\n",
        "3. SimpleIndexer: We need to specify the workspace parameter\n",
        "4. LMDBStorage: We also need to specify the workspace parameter. Furtheremore, the executor can be in parallel to the other branch. We can achieve this using `needs='gateway'`.\n",
        "5. A final executor which just waits for both branchs.\n",
        "\n",
        "After building the index Flow, we can plot it to verify that we're using the correct architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA11u6EwNQQh"
      },
      "outputs": [],
      "source": [
        "from jina import Flow\n",
        "index_flow = Flow().add(uses=YoloV5Segmenter, name='segmenter', uses_with={'device': device}) \\\n",
        "  .add(uses=CLIPImageEncoder, name='encoder', uses_with={'device': device, 'traversal_paths': '@c'}) \\\n",
        "  .add(uses=SimpleIndexer, name='chunks_indexer', workspace='workspace') \\\n",
        "  .add(uses=LMDBStorage, name='root_indexer', workspace='workspace', needs='gateway') \\\n",
        "  .add(name='wait_both', needs=['root_indexer', 'chunks_indexer'])\n",
        "index_flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNtexomTqqdg"
      },
      "source": [
        "Now it's time to index the dataset that we have downloaded. Actually, we will index images inside the `images` folder.\n",
        "This helper function will convert image files into Jina Documents and yield them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4jcfZwmPl7j"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from jina import Document\n",
        "\n",
        "def input_generator():\n",
        "    for filename in glob('images/*.jpg'):\n",
        "        doc = Document(uri=filename, tags={'filename': filename})\n",
        "        doc.load_uri_to_image_tensor()\n",
        "        yield doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPVk3IJxq-AL"
      },
      "source": [
        "The final step in this section is to send the input documents to the index Flow. Note that indexing can take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxxcAuxbRGqs"
      },
      "outputs": [],
      "source": [
        "with index_flow:\n",
        "    input_docs = input_generator()\n",
        "    index_flow.post(on='/index', inputs=input_docs, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hewIBnqurEQA"
      },
      "source": [
        "## Searching:\n",
        "Now, let's build the search Flow and use it in order to find sample query images.\n",
        "\n",
        "Our Flow contains the following executors:\n",
        "\n",
        "1. CLIPImageEncoder: It receives the device parameter. This time, since we want to encode root query documents, we specify that `'traversal_paths': '@r'`\n",
        "2. SimpleIndexer: We need to specify the workspace parameter\n",
        "3. SimpleRanker\n",
        "4. LMDBStorage: We need to specify the workspace parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ewwZfXRSDK"
      },
      "outputs": [],
      "source": [
        "from jina import Flow\n",
        "query_flow = Flow().add(uses=CLIPImageEncoder, name='encoder', uses_with={'device': device, 'traversal_paths': '@r'}) \\\n",
        "  .add(uses=SimpleIndexer, name='chunks_indexer', workspace='workspace') \\\n",
        "  .add(uses=SimpleRanker, name='ranker') \\\n",
        "  .add(uses=LMDBStorage, workspace='workspace', name='root_indexer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80AXRGIpsD6c"
      },
      "source": [
        "Let's plot our Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxvqtUxcbMbw"
      },
      "outputs": [],
      "source": [
        "query_flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3hlqP0sGyo"
      },
      "source": [
        "We create the following helper function in order to plot the result documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epm6N87fuJKY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_docs(docs):\n",
        "    for doc in docs[:3]:\n",
        "        plt.imshow(doc.tensor)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmdSwv7sOaO"
      },
      "source": [
        "Finally, we can start querying. We will use images inside the query folder.\n",
        "For each image, we will create a Jina Document. Then we send our documents to the query Flow and receive the response. \n",
        "\n",
        "For each query document, we can print the image and it's top 3 search results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b7RGWYUYeBx"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "with query_flow:\n",
        "    docs = [Document(uri=filename) for filename in glob.glob('query/*.jpg')]\n",
        "    for doc in docs:\n",
        "        doc.load_uri_to_image_tensor()\n",
        "    docs = query_flow.post('/search', docs, return_results=True)\n",
        "for doc in docs:\n",
        "    print('query:')\n",
        "    plt.imshow(doc.tensor)\n",
        "    plt.show()\n",
        "    print('results:')\n",
        "    show_docs(doc.matches)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "segmentation-image-3.0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}